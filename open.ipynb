{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  #This line imports the os module, which provides functions for interacting with the operating system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Langchain is a decentralized application that aims to revolutionize the language learning industry. It uses blockchain technology to create a global language learning ecosystem where users can connect with native speakers and language experts, access high-quality learning materials, and earn rewards for their contributions to the community.\n",
      "\n",
      "The platform offers a variety of features such as real-time language practice, personalized learning plans, and interactive lessons. It also utilizes AI technology to track user progress and provide personalized feedback.\n",
      "\n",
      "One of the key aspects of Langchain is its use of blockchain technology. This enables secure transactions, transparent reward distribution, and a decentralized governance system where users have a say in the development of the platform.\n",
      "\n",
      "Langchain is not just a language learning platform, but also a community where users can share their knowledge, culture, and experiences. It aims to break down language barriers and promote cultural exchange and understanding.\n",
      "\n",
      "Overall, Langchain is a promising project that has the potential to disrupt the traditional language learning industry and make language learning more effective, accessible, and rewarding for all. \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI #This line imports the OpenAI class from the langchain.llms module. It assumes you have a library called langchain installed that provides tools for working with large language models (LLMs).\n",
    "                                       #This line essentially creates a way to interact with OpenAI's API using the OpenAI class.\n",
    "llm=OpenAI()\n",
    "text=\"tell me about langchain ?\"\n",
    "print(llm.predict(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vijay\\openai\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Vijay\\openai\\env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Wie sind Sie?'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id = \"google/flan-t5-large\", model_kwargs={\"temperature\": 0.5})\n",
    "llm(\"translate English to German: How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese restaurant\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "llmh = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0})\n",
    "name = llmh.predict(\"I want to open a restaurant for Chinese food. Suggest a fency name for this.\")\n",
    "print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  ?'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmh(\"Translate english to hindi : How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1= OpenAI(temperature=0.9)   #temperature lies between 0-1 ( 0 means determisic and 1 mean highlly varaible) . This line creates an instance of the OpenAI class we imported earlier and assigns it to the variable llm.\n",
    "                                        #It also sets the temperature parameter to 0.9. Temperature is a parameter that controls the randomness of the generated text. A value of 0 means the model will generate the most likely responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"which is biggest company in the IT market\" #This line defines a variable named prompt and assigns a string to it. This string will be used as the starting point for the text generation by the LLM.\n",
    "prompt1=\"which is first company in the IT market\"\n",
    "prompt3=\"which is small company in the IT market\"\n",
    "#Eevry time we cant load these prompts one by one  directly to LLM that is Y  we use the PromptTemplate class and this class will take care  and create the prompt template like genrarlished which will be havinig palcehoolder in the prompt where u can just replace the first, biggest , small input and this prompt template makes the LLM understand easily \n",
    "#this is y we  cannot prompt directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= llm1.generate([prompt]*5)  # this gives five different compannies #This line defines a variable named prompt and assigns a string to it. This string will be used as the starting point for the text generation by the LLM.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\nAs of 2021, the biggest company in the IT market is Microsoft Corporation, with a market capitalization of over $2 trillion. Other major players in the IT market include Apple, Amazon, Alphabet (Google), and Facebook.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nMicrosoft\\n\\nAs of 2021, Microsoft is the biggest company in the IT market. It was founded in 1975 by Bill Gates and Paul Allen and has since grown to become one of the most valuable and influential companies in the world. Microsoft's main products and services include the Windows operating system, Microsoft Office suite, and the Microsoft Azure cloud computing platform. It also owns and operates popular products such as LinkedIn, Xbox, and Skype.  Microsoft's market capitalization as of 2021 is over $1.6 trillion, making it the largest IT company in terms of market value.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nAs of 2021, the biggest company in the IT market is Microsoft. ', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe biggest company in the IT market is currently Microsoft, with a market capitalization of over $1 trillion. Other large IT companies include Apple, Amazon, Alphabet (Google), and Facebook.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThere is no definitive answer to this question as it can vary depending on different factors such as revenue, market share, industry, and geographical location. However, some of the biggest and most well-known companies in the IT market include:\\n\\n1. Microsoft\\n2. Apple\\n3. Alphabet Inc. (Google)\\n4. Amazon\\n5. IBM\\n6. Facebook\\n7. Samsung\\n8. Intel\\n9. Cisco\\n10. Oracle', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 40, 'total_tokens': 356, 'completion_tokens': 316}, 'model_name': 'gpt-3.5-turbo-instruct'} run=[RunInfo(run_id=UUID('75ed3703-c7db-4db6-9521-3b14225b4f1f')), RunInfo(run_id=UUID('6441efc4-1f59-45ee-b261-033c5f74fd83')), RunInfo(run_id=UUID('91f077a3-d70f-41f5-8f4a-faa6ab072908')), RunInfo(run_id=UUID('f750e47e-d4ce-480f-98b5-a341518886f2')), RunInfo(run_id=UUID('214863bc-657e-485a-a238-6e3e4df50aef'))]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='\\n\\nAs of 2021, the biggest company in the IT market is Microsoft Corporation, with a market capitalization of over $2 trillion. Other major players in the IT market include Apple, Amazon, Alphabet (Google), and Facebook.' generation_info={'finish_reason': 'stop', 'logprobs': None} tell me about langchain ?\n",
      "text=\"\\n\\nMicrosoft\\n\\nAs of 2021, Microsoft is the biggest company in the IT market. It was founded in 1975 by Bill Gates and Paul Allen and has since grown to become one of the most valuable and influential companies in the world. Microsoft's main products and services include the Windows operating system, Microsoft Office suite, and the Microsoft Azure cloud computing platform. It also owns and operates popular products such as LinkedIn, Xbox, and Skype.  Microsoft's market capitalization as of 2021 is over $1.6 trillion, making it the largest IT company in terms of market value.\" generation_info={'finish_reason': 'stop', 'logprobs': None} tell me about langchain ?\n",
      "text='\\n\\nAs of 2021, the biggest company in the IT market is Microsoft. ' generation_info={'finish_reason': 'stop', 'logprobs': None} tell me about langchain ?\n",
      "text='\\n\\nThe biggest company in the IT market is currently Microsoft, with a market capitalization of over $1 trillion. Other large IT companies include Apple, Amazon, Alphabet (Google), and Facebook.' generation_info={'finish_reason': 'stop', 'logprobs': None} tell me about langchain ?\n",
      "text='\\n\\nThere is no definitive answer to this question as it can vary depending on different factors such as revenue, market share, industry, and geographical location. However, some of the biggest and most well-known companies in the IT market include:\\n\\n1. Microsoft\\n2. Apple\\n3. Alphabet Inc. (Google)\\n4. Amazon\\n5. IBM\\n6. Facebook\\n7. Samsung\\n8. Intel\\n9. Cisco\\n10. Oracle' generation_info={'finish_reason': 'stop', 'logprobs': None} tell me about langchain ?\n"
     ]
    }
   ],
   "source": [
    "for company_name in result.generations:     # This line starts a for loop that iterates over the generations attribute of the result object. The generations attribute likely contains a list of generated text snippets, one for each prompt provided.\n",
    "    print(company_name[0],text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line is inside the for loop. Here's what it does:\n",
    "\n",
    "company_name: This refers to the current item in the loop iteration. In this case, it's likely a data structure containing the generated text.\n",
    "\n",
    "[0]: This selects the first element (index 0) from company_name. Assuming the generated text is a single string, this selects the actual company name.\n",
    "\n",
    ".text: This might be an attribute that extracts the actual text from the data structure holding the generated response.\n",
    "\n",
    "print(company_name[0].text): This line prints the extracted company name to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next module is Differnt types of Prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prompt Template and Chainings.\n",
    "#Prompt Template refers to reproducible way to generate the prompt. it is text string that accept the parameter and uses them to format the final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate   #We're importing a class named PromptTemplate from the langchain library. This class helps us define templates (like our magic phrase) for generating text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain #We're importing the LLMChain class from the langchain.chains sub-module. This class is likely used to combine a prompt template with an LLM, potentially allowing for additional processing of generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a naming consultant for new companies. What is a good name for a {company} that makes {product}?\"\n",
    "\n",
    "#This line defines a variable named template. The value assigned is a string that acts as our prompt template. It includes placeholders ({company} and {product}) which will be replaced later with specific company and product information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're creating a PromptTemplate object (using the class imported in Line 1). We call the from_template method and pass the string template defined in the previous line. This creates a more structured representation of the prompt with its placeholders.\n",
    "\n",
    "Here, you're using the PromptTemplate tool you grabbed earlier (Line 1). You're giving it the magic phrase you just created (stored in template) and asking it to turn it into a proper template with those placeholders ready to be \n",
    "\n",
    "We use PromptTemplate to turn this phrase into a template with placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is another way of writing prompt templaate\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Indian food. Suggest a fency name for this.\n"
     ]
    }
   ],
   "source": [
    "#this is another way of writing prompt templaate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "p = prompt_template_name.format(cuisine=\"Indian\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're creating an instance of the OpenAI class (imported in Line 2). This establishes a connection to the LLM. We also set the temperature parameter to 0.9. Temperature in this context controls the randomness of the generated text. A value closer to 1 leads to potentially more creative but less likely outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here chains comes into the pictures, Chianning concept of langchain  can beyond of single LLM call and involves sequence of calls and most comman  chain is LLM chain = LLM + prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here, we're creating an instance of the LLMChain class (imported in Line 3). It takes two arguments:\n",
    "\n",
    "llm: This is set to the llm object created in Line 6, providing the LLM to be used.\n",
    "\n",
    "prompt: This is set to the prompt object created in Line 5, defining the template for the prompt. Essentially, this chain combines the LLM and the prompt template, preparing them to work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line calls the run method on the chain object created in Line 7. This method likely triggers the LLM to generate text based on the provided prompt template and the inserted values.\n",
    "\n",
    "The argument to run is a dictionary containing two key-value pairs:\n",
    "\n",
    "\"company\": This is set to \"ABC Startup\", which will replace the {company} placeholder in the prompt template.\n",
    "\n",
    "\"product\": This is set to \"colorful socks\", which will replace the {product} placeholder in the prompt template.\n",
    "\n",
    "The print statement displays the output from the LLM, which should be a company name suggestion based on the provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Toes Socks Co. \n"
     ]
    }
   ],
   "source": [
    "print(chain.run({\"company\": \"ABC Startup\", \"product\": \"colorful socks\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Sequencial Chains . It means  here we chain the output generated of the one LLM and we can use that output as   input prompt to other LLM  to generate the output...this is done by lanchain chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate  #promptTemplate helps us build templates for the phrases we want the LLM to process.\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "template = \"What is a good name for a company that makes {product}?\"\n",
    "first_prompt = PromptTemplate.from_template(template)\n",
    "first_chain = LLMChain(llm=llm, prompt=first_prompt)  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chain provides the company name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second second chain provide the catch prahse using the company name provide by the first chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done by using SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_template = \"Write a catch phrase for the following company: {company_name}\"\n",
    "second_prompt = PromptTemplate.from_template(second_template)  #We create a PromptTemplate object for this slogan.\n",
    "second_chain = LLMChain(llm=llm, prompt=second_prompt) #We create a new LLMChain object (second_chain) connecting the LLM (llm) with the second prompt (second_prompt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a SimpleSequentialChain object, which allows us to chain multiple LLMChain objects. Here, we chain the two individual prompt chains (first_chain and second_chain). We set verbose=True to see detailed information during execution (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the two pieces we created (company name generation and slogan generation) into one bigger chain. This allows us to get both the company name and the slogan in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the two pieces we created (company name generation and slogan generation) into one bigger chain. This allows us to get both the company name and the slogan in a single step.\n",
    "\n",
    "\n",
    "We run the entire chain, feeding in the information about the product (colorful socks). This will trigger the LLM to first generate a company name and then a slogan based on that name. Finally, we print the complete catchphrase, which includes both the company name and the slogan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "\"Rainbow Threads\" or \"Vibrant Socks Co.\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "\"Step into a world of color with Rainbow Threads!\" \n",
      "\"Add a pop of color to your step with Vibrant Socks Co.!\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\"Step into a world of color with Rainbow Threads!\" \n",
      "\"Add a pop of color to your step with Vibrant Socks Co.!\"\n"
     ]
    }
   ],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[first_chain, second_chain], verbose=True) #This can be done by using SimpleSequentialChain where you connect both the chains\n",
    "catchphrase = overall_chain.run(\"colorful socks\")      # here we give  firast variable in the first chain and it gave us the company name and automatically it took  comapny name as second variable\n",
    "print(catchphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means  here we can chain the output generated of the one LLM and we can use that output as  input prompt to other LLM  to generate the output...this is done by lanchain chain (Seqsequnce chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is using Squentialchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is using Squentialchain\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
    "\n",
    "\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vijay\\openai\\env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'indian', 'restaurant_name': '\\n\\n\"Maharaja\\'s Palace\"', 'menu_items': '\\n\\n1. Tandoori Chicken: Juicy chicken marinated in traditional Indian spices and roasted in a clay oven.\\n\\n2. Butter Naan: Soft and fluffy Indian flatbread brushed with butter.\\n\\n3. Saag Paneer: Creamy spinach and cheese curry.\\n\\n4. Chicken Tikka Masala: Tender chicken in a rich and flavorful tomato-based sauce.\\n\\n5. Vegetable Biryani: Fragrant basmati rice cooked with assorted vegetables and spices.\\n\\n6. Lamb Rogan Josh: Slow-cooked lamb in a spicy and aromatic gravy.\\n\\n7. Dal Makhani: Creamy black lentils cooked with butter and aromatic spices.\\n\\n8. Aloo Gobi: Spiced cauliflower and potatoes stir-fried with onions and tomatoes.\\n\\n9. Mango Lassi: Refreshing yogurt-based drink with a hint of sweet mango.\\n\\n10. Gulab Jamun: Warm and syrupy milk-based dessert balls flavored with cardamom.'}\n"
     ]
    }
   ],
   "source": [
    "print(chain({\"cuisine\": \"indian\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions Agent\n",
    "here we create actin agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pprint\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import load_tools, initialize_agent  # This helps us create an \"agent\" object that combines these tools with a powerful LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When was the 3rd president of the United States born? What is that year raised to the power of 3?\" # We create a variable named prompt and assign it a string value. This string represents the question we want the agent to answer. The question requires two steps: finding the birth year and then calculating the cube of that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm) #We call the load_tools function to equip our agent with specific skills. We provide a list of tool names as arguments:wikipedia & llm-math\" #We also pass the llm object as an argument, ensuring the tools can communicate with the LLM for processing information.\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)  #We call the initialize_agent function. This function combines the loaded tools (tools), the LLM (llm), and creates an \"agent\" object. Here's what the other arguments mean:# agent=\"zero-shot-react-description\": This tells the agent it can react to new prompts (like our question) without prior training and provide descriptive answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the birth year of the 3rd president of the United States and then use a calculator to raise that year to the power of 3.\n",
      "Action: Wikipedia\n",
      "Action Input: \"3rd president of the United States\"\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the birth year of the 3rd president of the United States and then use a calculator to raise that year to the power of 3.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Thomas Jefferson\"\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the birth year of the 3rd president of the United States and then use a calculator to raise that year to the power of 3.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Thomas Jefferson birth year\"\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the birth year of the 3rd president of the United States and then use a calculator to raise that year to the power of 3.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Birth of Thomas Jefferson\"\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the birth year of the 3rd president of the United States and then use a calculator to raise that year to the power of 3.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Thomas Jefferson birth\"\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the birth year of the 3rd president of the United States and then use a calculator to raise that year to the power of 3.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Birth of Thomas Jefferson\"\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the birth year of the 3rd president of the United States and then use a calculator to raise that year to the power of 3.\n",
      "Action: Calculator\n",
      "Action Input: \"1743^3\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 5295319407\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 5295319407\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5295319407'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(prompt) #We call the run method on the agent object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes will be a question where LLM or this case  agent wont know what  to do and will ask human to provide he input , something personal, something may be about your  friend or your parents or any thing where  the human input  might be needed. You can use human as the tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType   #This AgentType class likely defines different types of agents you can use in your chatbot interactions, each with specific behavior or capabilities.\n",
    "from langchain.chat_models import ChatOpenAI #This ChatOpenAI class helps you create chatbots powered by OpenAI's large language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0) #You're creating an object named llm of the ChatOpenAI class (imported in line 1). The ChatOpenAI class probably handles communication with OpenAI's language models.\n",
    "math_llm = OpenAI(temperature=0.0) #You're creating another object named math_llm but of the OpenAI class (not ChatOpenAI). This suggests there might be a more generic OpenAI class for general LLM interaction, and ChatOpenAI might be a subclass designed specifically for chatbots.\n",
    "tools = load_tools(\n",
    "    [\"human\", \"llm-math\"],\n",
    "    llm=math_llm,\n",
    ")\n",
    "#Explanation: You're calling a function named load_tools (likely from the langchain library, not explicitly imported here). This function seems to be responsible for loading different tools or components for your chatbot.\n",
    "#The first argument [\"human\", \"llm-math\"] is a list containing strings, possibly representing the types of tools you want to include: \"human\" (likely for human interaction) and \"llm-math\" (presumably for the math_llm object you created earlier).\n",
    "#The second argument llm=math_llm explicitly provides the math_llm object to the load_tools function, potentially for situations where the \"llm-math\" tool needs access to that specific LLM instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're calling a function named initialize_agent (also likely from langchain). This function probably takes several arguments to set up the core logic of your chatbot.\n",
    "\n",
    "You're passing the tools object created in line 5, containing the different components for the chatbot.\n",
    "\n",
    "You're passing the llm object (line 3) as another argument, likely providing the main language model for the chatbot to use.\n",
    "\n",
    "You're setting the agent parameter to AgentType.ZERO_SHOT_REACT_DESCRIPTION. This suggests langchain might provide different agent types with varying behaviors. Here, you're choosing a type that likely reacts and describes based on the input without additional training (zero-shot learning).\n",
    "\n",
    "You're setting the verbose parameter to True, possibly indicating you want more detailed output or logging during the chatbot interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent_chain.run(\"What's my friend Andi's surname?\") #you're calling the run method on the agent_chain object\n",
    "#This method likely starts the chatbot interaction. You're providing the question \"What's my friend Andi's surname?\" as input, triggering the chatbot to process it, potentially using the loaded tools and language models, and generate a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan and Execute Agents\n",
    "\n",
    "#first plan the series of the steps and then execute the each one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#search for the  loaction \n",
    "\n",
    "#identify the country\n",
    "\n",
    "#Search for the population of the country\n",
    "\n",
    "#raise the population  to 5\n",
    "\n",
    "#return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM always  has model for thr planning and model execution and then both plan and execute the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan and execute agent are best for complex or long running task whereas long term obeective or focus you wann think of these steps beforehand then execute them one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Where will the next summer olympics be hosted? What is the population of that country raised to the 5 power?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.plan_and_execute import (\n",
    "    PlanAndExecute,\n",
    "    load_agent_executor,\n",
    "    load_chat_planner,\n",
    ")\n",
    "from langchain import SerpAPIWrapper, LLMMathChain  #SerpAPI helps us the acess the google serach\n",
    "from langchain.agents.tools import Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['SERPAPI_API_KEY'] = 'a48dcb234de143fcd9a28be6dd7c64aab4ff844560aceb7b1acce1cbe118a45c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SerpAPIWrapper()\n",
    "llm = OpenAI(temperature=0)\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\",\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0)   # here use chatopenAI model used beaused planner & executer  should use memory i.e. chat models to leverage previous information \n",
    "planner = load_chat_planner(model)\n",
    "executor = load_agent_executor(model, tools, verbose=True)\n",
    "agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new PlanAndExecute chain...\u001b[0m\n",
      "steps=[Step(value='Find out where the next summer Olympics will be hosted.'), Step(value='Determine the country hosting the next summer Olympics.'), Step(value='Look up the population of that country.'), Step(value='Raise the population of that country to the 5th power.'), Step(value='Provide the result of the population raised to the 5th power.'), Step(value=\"Given the above steps taken, please respond to the user's original question.\\n\")]\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user's current objective is to find out where the next summer Olympics will be hosted. Since the previous steps have already determined that the next summer Olympics will be hosted in Paris, France, I can provide this information directly to the user.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The next summer Olympics will be hosted in Paris, France.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Find out where the next summer Olympics will be hosted.\n",
      "\n",
      "Response: The next summer Olympics will be hosted in Paris, France.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user's current objective is to determine the country hosting the next summer Olympics, which we know is France.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"France\"\n",
      "}\n",
      "``` \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Determine the country hosting the next summer Olympics.\n",
      "\n",
      "Response: France\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The population of the country in question is France, and it has been previously determined to be approximately 67.75 million in 2021.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The population of France in 2021 is approximately 67.75 million.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Look up the population of that country.\n",
      "\n",
      "Response: The population of France in 2021 is approximately 67.75 million.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The current objective is to raise the population of France to the 5th power.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"67.75^5\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "67.75^5\u001b[32;1m\u001b[1;3m```text\n",
      "67.75**5\n",
      "```\n",
      "...numexpr.evaluate(\"67.75**5\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m1427402646.8271484\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1427402646.8271484\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe population of France raised to the 5th power is approximately 1,427,402,646.83.\n",
      "Thought: I have the final answer to provide to the user.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The population of France raised to the 5th power is approximately 1,427,402,646.83.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Raise the population of that country to the 5th power.\n",
      "\n",
      "Response: The population of France raised to the 5th power is approximately 1,427,402,646.83.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The population of France has already been raised to the 5th power in the previous steps, and the result is approximately 1,427,402,646.83. I should provide this as the final answer to the user's current objective.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The population of France raised to the 5th power is approximately 1,427,402,646.83.\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Provide the result of the population raised to the 5th power.\n",
      "\n",
      "Response: The population of France raised to the 5th power is approximately 1,427,402,646.83.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user's original question is to respond based on the steps taken, which involve finding the population of France and raising it to the 5th power.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The population of France raised to the 5th power is approximately 1,427,402,646.83.\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Given the above steps taken, please respond to the user's original question.\n",
      "\n",
      "\n",
      "Response: The population of France raised to the 5th power is approximately 1,427,402,646.83.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The population of France raised to the 5th power is approximately 1,427,402,646.83.\n"
     ]
    }
   ],
   "source": [
    "print(agent.run(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory and chat bots\n",
    "\n",
    "This memory will be saved in local memory like RAM or cache memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your OpenAI API key   # Function calling \n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# This function takes a text snippet and returns its sentiment (positive, negative, or neutral)\n",
    "def get_sentiment(text):\n",
    "  \"\"\"\n",
    "  This function analyzes the sentiment of a text snippet.\n",
    "\n",
    "  Args:\n",
    "      text: The text snippet to analyze.\n",
    "\n",
    "  Returns:\n",
    "      A string representing the sentiment (positive, negative, or neutral).\n",
    "  \"\"\"\n",
    "  \n",
    "  \n",
    "  response = openai.Completion.create(\n",
    "      engine=\"gpt-3.5-turbo\",\n",
    "      prompt=\"Analyze the sentiment of the following text: '\" + text + \"'\",\n",
    "      max_tokens=1,\n",
    "      n=1,\n",
    "      stop=None,\n",
    "      temperature=0.9,\n",
    "      )\n",
    "  sentiment = response.choices[0].text.strip()\n",
    "  return sentiment\n",
    "\n",
    "# Example usage: Pass some text to the get_sentiment function\n",
    "text_to_analyze = \"Today is a beautiful day!\"\n",
    "sentiment = get_sentiment(text_to_analyze)\n",
    "\n",
    "print(\"The sentiment of the text is:\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
