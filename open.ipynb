{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  #This line imports the os module, which provides functions for interacting with the operating system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI #This line imports the OpenAI class from the langchain.llms module. It assumes you have a library called langchain installed that provides tools for working with large language models (LLMs).\n",
    "                                       #This line essentially creates a way to interact with OpenAI's API using the OpenAI class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm\u001b[38;5;241m=\u001b[39mOpenAI(\u001b[43mpredict\u001b[49m(text))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "llm=OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"tell me about langchain ?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.predict(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1= OpenAI(temperature=0.9)   #temperature lies between 0-1 ( 0 means determisic and 1 mean highlly varaible) . This line creates an instance of the OpenAI class we imported earlier and assigns it to the variable llm.\n",
    "                                        #It also sets the temperature parameter to 0.9. Temperature is a parameter that controls the randomness of the generated text. A value of 0 means the model will generate the most likely responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"which is biggest company in the IT market\" #This line defines a variable named prompt and assigns a string to it. This string will be used as the starting point for the text generation by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= llm1.generate([prompt]*5)  # this gives five different compannies #This line defines a variable named prompt and assigns a string to it. This string will be used as the starting point for the text generation by the LLM.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\nThe biggest company in the IT market is currently Microsoft, with a market capitalization of over $1 trillion. Other major players in the industry include Apple, Amazon, Alphabet (Google), and Facebook.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\\nAs of 2021, the biggest company in the IT market is Microsoft Corporation.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nAs of 2021, the biggest company in the IT market is Microsoft. Other major players in the industry include Apple, Amazon, Alphabet (Google), and Facebook. ', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='.\"Microsoft\" is considered the biggest company in the IT market. Other notable companies include Google, Apple, and Amazon.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nMicrosoft Corporation is currently the biggest company in the IT market, with a market capitalization of over $1 trillion. Other major players include Apple, Amazon, Alphabet (Google), and Facebook. ', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 200, 'prompt_tokens': 40, 'completion_tokens': 160}, 'model_name': 'gpt-3.5-turbo-instruct'} run=[RunInfo(run_id=UUID('4dcde37b-61e9-4f44-8033-731dca601e02')), RunInfo(run_id=UUID('006ba34a-e5df-4975-8692-ea0e7a6862f8')), RunInfo(run_id=UUID('d7cd3fbd-50d4-42b6-ac4f-58a10b6b93ca')), RunInfo(run_id=UUID('a5daac48-6710-4abc-b1e5-3b87b72a7fd1')), RunInfo(run_id=UUID('489e24f8-f1ab-464f-8d4b-d8dbff6e261c'))]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='\\n\\nThe biggest company in the IT market is currently Microsoft, with a market capitalization of over $1 trillion. Other major players in the industry include Apple, Amazon, Alphabet (Google), and Facebook.' generation_info={'finish_reason': 'stop', 'logprobs': None} tell me about langchain ?\n",
      "text='\\n\\n\\nAs of 2021, the biggest company in the IT market is Microsoft Corporation.' generation_info={'finish_reason': 'stop', 'logprobs': None} tell me about langchain ?\n",
      "text='\\n\\nAs of 2021, the biggest company in the IT market is Microsoft. Other major players in the industry include Apple, Amazon, Alphabet (Google), and Facebook. ' generation_info={'finish_reason': 'stop', 'logprobs': None} tell me about langchain ?\n",
      "text='.\"Microsoft\" is considered the biggest company in the IT market. Other notable companies include Google, Apple, and Amazon.' generation_info={'finish_reason': 'stop', 'logprobs': None} tell me about langchain ?\n",
      "text='\\n\\nMicrosoft Corporation is currently the biggest company in the IT market, with a market capitalization of over $1 trillion. Other major players include Apple, Amazon, Alphabet (Google), and Facebook. ' generation_info={'finish_reason': 'stop', 'logprobs': None} tell me about langchain ?\n"
     ]
    }
   ],
   "source": [
    "for company_name in result.generations:     # This line starts a for loop that iterates over the generations attribute of the result object. The generations attribute likely contains a list of generated text snippets, one for each prompt provided.\n",
    "    print(company_name[0],text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line is inside the for loop. Here's what it does:\n",
    "\n",
    "company_name: This refers to the current item in the loop iteration. In this case, it's likely a data structure containing the generated text.\n",
    "\n",
    "[0]: This selects the first element (index 0) from company_name. Assuming the generated text is a single string, this selects the actual company name.\n",
    "\n",
    ".text: This might be an attribute that extracts the actual text from the data structure holding the generated response.\n",
    "\n",
    "print(company_name[0].text): This line prints the extracted company name to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT module is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prompt Template and Chainings.\n",
    "#Prompt Template refers to reproducible way to generate the prompt. it is text string that accept the parameter and uses them to format the final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate   #We're importing a class named PromptTemplate from the langchain library. This class helps us define templates (like our magic phrase) for generating text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain #We're importing the LLMChain class from the langchain.chains sub-module. This class is likely used to combine a prompt template with an LLM, potentially allowing for additional processing of generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a naming consultant for new companies. What is a good name for a {company} that makes {product}?\"\n",
    "\n",
    "#This line defines a variable named template. The value assigned is a string that acts as our prompt template. It includes placeholders ({company} and {product}) which will be replaced later with specific company and product information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're creating a PromptTemplate object (using the class imported in Line 1). We call the from_template method and pass the string template defined in the previous line. This creates a more structured representation of the prompt with its placeholders.\n",
    "\n",
    "Here, you're using the PromptTemplate tool you grabbed earlier (Line 1). You're giving it the magic phrase you just created (stored in template) and asking it to turn it into a proper template with those placeholders ready to be \n",
    "\n",
    "We use PromptTemplate to turn this phrase into a template with placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're creating an instance of the OpenAI class (imported in Line 2). This establishes a connection to the LLM. We also set the temperature parameter to 0.9. Temperature in this context controls the randomness of the generated text. A value closer to 1 leads to potentially more creative but less likely outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here chains comes into the pictures, Chianning concept of langchain  can beyond of single LLM call and involves sequence of calls and most comman  chain is LLM chain = LLM + prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here, we're creating an instance of the LLMChain class (imported in Line 3). It takes two arguments:\n",
    "\n",
    "llm: This is set to the llm object created in Line 6, providing the LLM to be used.\n",
    "\n",
    "prompt: This is set to the prompt object created in Line 5, defining the template for the prompt. Essentially, this chain combines the LLM and the prompt template, preparing them to work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line calls the run method on the chain object created in Line 7. This method likely triggers the LLM to generate text based on the provided prompt template and the inserted values.\n",
    "\n",
    "The argument to run is a dictionary containing two key-value pairs:\n",
    "\n",
    "\"company\": This is set to \"ABC Startup\", which will replace the {company} placeholder in the prompt template.\n",
    "\n",
    "\"product\": This is set to \"colorful socks\", which will replace the {product} placeholder in the prompt template.\n",
    "\n",
    "The print statement displays the output from the LLM, which should be a company name suggestion based on the provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vijay\\openai\\env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Rainbow Threads Co.\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.run({\"company\": \"ABC Startup\", \"product\": \"colorful socks\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Sequencial Chains . It means  here we chain the output generated of the one LLM and we can use that output as   input prompt to other LLM  to generate the output...this is done by lanchain chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate  #promptTemplate helps us build templates for the phrases we want the LLM to process.\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "template = \"What is a good name for a company that makes {product}?\"\n",
    "first_prompt = PromptTemplate.from_template(template)\n",
    "first_chain = LLMChain(llm=llm, prompt=first_prompt)  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chain provides the company name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second second chain provide the catch prahse using the company name provide by the first chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done by using SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_template = \"Write a catch phrase for the following company: {company_name}\"\n",
    "second_prompt = PromptTemplate.from_template(second_template)  #We create a PromptTemplate object for this slogan.\n",
    "second_chain = LLMChain(llm=llm, prompt=second_prompt) #We create a new LLMChain object (second_chain) connecting the LLM (llm) with the second prompt (second_prompt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a SimpleSequentialChain object, which allows us to chain multiple LLMChain objects. Here, we chain the two individual prompt chains (first_chain and second_chain). We set verbose=True to see detailed information during execution (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the two pieces we created (company name generation and slogan generation) into one bigger chain. This allows us to get both the company name and the slogan in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the two pieces we created (company name generation and slogan generation) into one bigger chain. This allows us to get both the company name and the slogan in a single step.\n",
    "\n",
    "\n",
    "We run the entire chain, feeding in the information about the product (colorful socks). This will trigger the LLM to first generate a company name and then a slogan based on that name. Finally, we print the complete catchphrase, which includes both the company name and the slogan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "\"Rainbow Threads\" or \"Vibrant Socks Co.\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "\"Step into a world of color with Rainbow Threads!\" \n",
      "\"Add a pop of color to your step with Vibrant Socks Co.!\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\"Step into a world of color with Rainbow Threads!\" \n",
      "\"Add a pop of color to your step with Vibrant Socks Co.!\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[first_chain, second_chain], verbose=True) #This can be done by using SimpleSequentialChain where you connect both the chains\n",
    "catchphrase = overall_chain.run(\"colorful socks\")      # here we give  firast variable in the first chain and it gave us the company name and automatically it took  comapny name as second variable\n",
    "print(catchphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means  here we can chain the output generated of the one LLM and we can use that output as  input prompt to other LLM  to generate the output...this is done by lanchain chain (Seqsequnce chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pprint\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import load_tools, initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When was the 3rd president of the United States born? What is that year raised to the power of 3?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your OpenAI API key\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# This function takes a text snippet and returns its sentiment (positive, negative, or neutral)\n",
    "def get_sentiment(text):\n",
    "  \"\"\"\n",
    "  This function analyzes the sentiment of a text snippet.\n",
    "\n",
    "  Args:\n",
    "      text: The text snippet to analyze.\n",
    "\n",
    "  Returns:\n",
    "      A string representing the sentiment (positive, negative, or neutral).\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "      engine=\"gpt-3.5-turbo\",\n",
    "      prompt=\"Analyze the sentiment of the following text: '\" + text + \"'\",\n",
    "      max_tokens=1,\n",
    "      n=1,\n",
    "      stop=None,\n",
    "      temperature=0.9,\n",
    "      )\n",
    "  sentiment = response.choices[0].text.strip()\n",
    "  return sentiment\n",
    "\n",
    "# Example usage: Pass some text to the get_sentiment function\n",
    "text_to_analyze = \"Today is a beautiful day!\"\n",
    "sentiment = get_sentiment(text_to_analyze)\n",
    "\n",
    "print(\"The sentiment of the text is:\", sentiment)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
